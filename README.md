# DAugBERT: A Causality Prediction Pipeline Using Back-translation Data Augmentation with BERT

Establishing causal connections between correlating events lies at the heart of scientific study, from the natural to social sciences. However, despite the critical role that causal connections play in the sciences, linguistic causality has not been given quite the same importance in Natural Language Processing (NLP). In this paper, we treat causality as a predictive task and attempt to automatically label causal language in the United Nations General Debates. We make use of BERT, a multi-layer bidirectional transformer, trained on a manually annotated dataset. Our model, trained on the re-balanced training set, is able to label causality on the test set with up to an 82% accuracy.

### Authors: Bitan Biswas, Hanna BÃ©chara, Slava Jankin, Paulina Garcia Corral

